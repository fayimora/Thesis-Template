
\chapter{Conclusion}
\label{ch:conclusions}

\section{Summary of Report Achievements}
In Chapter~\ref{cha:data-classification}, we were able to create a model to classify tweets into
relevant and non relevant groups. We learnt that accuracy is not a good measure for a classifier's
performance. As a result we decided to use the Area Under the Receiver Operating Characteristic
Curve(AUC) as an evaluation metric. We also used k-fold cross validation to evaluate the performance
of our classifier on an unseen dataset. The average AUC of our
initial(\Sectionref{sec:training-initial-classifier}) and best
model(\Sectionref{sec:exhaustive-grid-search}) were 0.74 and 0.85, respectively. This means we
achieved a 13.8\% increase in AUC.

In other to create our initial training set, we used a web application which displayed a number of
tweets and options to classify them into relevant and irrelevant groups(\Figref{fig:labeller}). The
application was created for this project but can be used in other similar projects. The only
requirement is that tweets are stored in a MongoDB database. It can also be extended easily to
support extra features if required.

In Chapter~\ref{cha:topic-modelling}, we created two topic models, one with 30 topics and the other
with 40 topics. We empirically analysed and evaluated some of the topics generated by the 30 topics
model. We discovered that the topic model was able to correctly place the right tweets under the
right topics. We also learnt that LDA can sometimes merge two or three topics into
one(\Sectionref{sec:topic-9}, \Sectionref{sec:topic-13}, \Sectionref{sec:topic-27}).

Rather than re-evaluating the topics generated by the 40 topics model, we compared them to that
generated by the 30 topics model. This is in a effort to find out if there will be overlapping
topics considering the same dataset was used. Fortunately, we were able to find a few overlapping
topics(\Tableref{tab:similar-topics}).

\section{Applications}
As discussed in Chapter~\ref{cha:introduction}, companies who are looking to get customers'
feedback without putting any pressure on the customers can use our research. They could gather data
from multiple social platforms, train a classifier to filter out irrelevant tweets and then run
topic modelling on the data. This project itself uses one of these companies as a case study.

Aside from companies looking to get feedback, sporting bodies like the Football Association in
England or the International Olympics Committee(IOC) could also benefit from this research. For
instance, during the London 2012 Summer Olympics, Twitter claimed that 9.66 million tweets were sent
during the opening ceremony\footnote{\url{https://blog.twitter.com/en-gb/2012/today-on-twitter-14}}.
This is a lot of data that could be useful in planning similar future events. These tweets could be
passed through a topic modelling system and the main themes discussed during the ceremony will be
detected. The output can then be used to improve the next Olympics.


\section{Future Work}
We chose the naive Bayes classifier for this project mainly because of its simplicity. It would be
interesting to find out how a Support Vector Machine will perform on our dataset compared to our
Bayes classifier.

Usually, detecting topics in a dataset is only a step to achieving other goals. One interesting
experiment would be to find a way of knowing the overall sentiments of a topic. Analysing the
sentiment of a single tweet might be easy but analysing the sentiments of a topic might prove more
difficult. This is because our topic model does not categorize a tweet under just one topic but
rather, each tweet is made up of varying proportions of different topics.

